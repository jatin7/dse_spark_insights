#/bin/bash

# pull collectd spark
git clone https://github.com/signalfx/collectd-spark

mkdir /usr/share/dse/collectd/collectd-spark
cp collectd-spark/spark_plugin.py /usr/share/dse/collectd/collectd-spark/


# add config for collectd collectd spark plugin
ln -s /usr/lib/python2.7/ /usr/share/dse/collectd/usr/lib/python2.7

MASTER_URL=$(curl node0_ext:7080 -LIs | grep Location | awk -F' ' '{print $2}' | awk -F':' '{print $1 ":" $2}'
)

mkdir /etc/dse/collectd/
cat << EOF > /etc/dse/collectd/10-spark.conf
LoadPlugin python
<Plugin python>
  ModulePath "/tmp/spark-insights/collectd-spark"

  Import spark_plugin

  <Module spark_plugin>
  MetricsURL "$MASTER_URL"
  MasterPort 7080
  WorkerPorts 7081
  Applications "True"
  Master "$MASTER_URL:7080"
  Cluster "Standalone"
  </Module>
</Plugin>
LoadPlugin write_prometheus

<Plugin write_prometheus>
 Port “9103”
</Plugin>
EOF

# turn on collectd
dsetool insights_config --mode ENABLED_WITH_LOCAL_STORAGE

# grafana and prometheus

export PROMETHEUS_DATA_DIR=/mnt/ephemeral/prometheus
export GRAFANA_DATA_DIR=/mnt/ephemeral/grafana

mkdir $PROMETHEUS_DATA_DIR
mkdir $GRAFANA_DATA_DIR

chmod 777 $PROMETHEUS_DATA_DIR
chmod 777 $GRAFANA_DATA_DIR

git clone https://github.com/datastax/dse-metric-reporter-dashboards.git
cd dse-metric-reporter-dashboards

cat /etc/hosts | grep node | grep -v ext| grep -v allnodes | awk -F' ' '{print $1 ":9103"}'  | jq -R . | jq -s ".| [{targets:[.[]], labels:{cluster: \"test_cluster\" }}]" > prometheus/tg_dse.json

pip install docker-compose

docker-compose up &
